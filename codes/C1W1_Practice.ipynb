{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation\n",
    "<img align=\"left\" src=\"../images/C1_W1_L3_S1_Lecture_b.png\"    style=\" width:96%; height:250px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_L3_S1_trainingdata.png\"    style=\" width:47%; height:200px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_L3_S1_model.png\"    style=\" width:47%; height:200px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_L3_S2_Lecture_b.png\"    style=\" width:96%; height:250px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_Lab02_GoalOfRegression.PNG\"    style=\" width:96%; height:200px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_L4_S1_Lecture_GD.png\"    style=\" width:96%; height:250px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_Lab03_lecture_slopes.PNG\"    style=\" width:47%; height:200px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_Lab03_lecture_learningrate.PNG\"    style=\" width:47%; height:200px; padding: 10px;  \" />\n",
    "<img align=\"left\" src=\"../images/C1_W1_Lab03_alpha_too_big.PNG\"    style=\" width:96%; height:250px; padding: 10px;  \" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients, plt_stationary, plt_update_onclick, soup_bowl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.csv')\n",
    "df.columns = df.columns.str.lower().str.replace(' ','_')\n",
    "\n",
    "list_of_head = list(df.dtypes[df.dtypes != 'object'].index)\n",
    "# list_of_head\n",
    "df['engine_hp'].fillna(round(df['engine_hp'].mean(),2),inplace=True)\n",
    "x_feature = df['engine_hp'].iloc[:100]\n",
    "y_target = df['msrp'].iloc[:100]\n",
    "# df['msrp'].isnull().sum()\n",
    "# for col in list_of_head:\n",
    "#     df[col] = df[col].str.lower().str.replace(' ', '_')\n",
    "# for col in df.columns:\n",
    "#     print(df[col].unique()[:5])\n",
    "#     print(df[col].nunique())\n",
    "#     print()\n",
    "# x_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cost function \n",
    "w11 = 100\n",
    "b11 = 51\n",
    "x11 = np.array([1.0, 2.0, 3.0])\n",
    "y11 = np.array([150, 250, 350])\n",
    "def cost_function(w, b, x, y):\n",
    "    print(x)\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = (w * x[i]) + b\n",
    "        cost += (f_wb - y[i]) ** 2\n",
    "    j_wb = (1/(2*m)) * cost\n",
    "    return j_wb\n",
    "cost_function(w11, b11, x11, y11)\n",
    "# print(j_wb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.1\"></a>\n",
    "## Gradient descent summary\n",
    "So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, *gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 1.0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute gradient\n",
    "def compute_gradient(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = (w * x[i]) + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db\n",
    "compute_gradient(x11, y11, w11, b11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
